#Install Cloudera Repo
cd /etc/yum.repos.d/
wget http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo

#Install components
sudo yum install hadoop-yarn-resourcemanager
sudo yum clean all; sudo yum install hadoop-hdfs-namenode 
sudo yum clean all; sudo yum install hadoop-hdfs-secondarynamenode
sudo yum clean all; sudo yum install hadoop-yarn-nodemanager hadoop-hdfs-datanode hadoop-mapreduce
sudo yum clean all; sudo yum install hadoop-client


#Config
#create base config
sudo cp -r /etc/hadoop/conf.empty /etc/hadoop/conf.mrlo_cluster
sudo alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.mrlo_cluster 50
sudo alternatives --set hadoop-conf /etc/hadoop/conf.mrlo_cluster

#To Display
sudo alternatives --display hadoop-conf

#### HDFS ####
#Configure core-site-xml
<property>
 <name>fs.defaultFS</name>
 <value>hdfs://fintonicdev1.cloudapp.net:8020</value>
</property>
<property>
 <name>hadoop.proxyuser.mapred.groups</name>
 <value>*</value>
</property> 
<property>
 <name>hadoop.proxyuser.mapred.hosts</name>
 <value>*</value>
</property> 

#Configure hdfs-site-xml
<property>
     <name>dfs.datanode.data.dir</name>
     <value>file:///mnt/resource/data/1/dfs/dn</value>
</property>



<property>
 <name>dfs.permissions.superusergroup</name>
 <value>hadoop</value>
</property>

<property>
  <name>dfs.namenode.http-address</name>
  <value>fintonicdev1.cloudapp.net:50070</value>
  <description>
    The address and the base port on which the dfs NameNode Web UI will listen.
  </description>
</property>

<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
</property>


#Formating hdfs dirs
mkdir -p /mnt/resource/data/1/dfs/dn
chown -R hdfs:hdfs /mnt/resource/data/

sudo -u hdfs hdfs namenode -format


# After starting hdfs services
sudo -u hdfs hadoop fs -mkdir /tmp
sudo -u hdfs hadoop fs -chmod -R 1777 /tmp


#### YARN #####
#mapred-site.xml
<property>
 <name>mapreduce.framework.name</name>
 <value>yarn</value>
</property> 
<property>
    <name>yarn.app.mapreduce.am.staging-dir</name>
    <value>/user</value>
</property>


#yarn-site.xml
<property>
 <name>mapreduce.jobhistory.address</name>
 <value>fintonicdev1.cloudapp.net:10020</value>
</property> 
<property>
 <name>mapreduce.jobhistory.webapp.address</name>
 <value>fintonicdev1.cloudapp.net:19888</value>
</property> 

#staging dir
sudo -u hdfs hadoop fs -mkdir -p /user/history
sudo -u hdfs hadoop fs -chmod -R 1777 /user/history
sudo -u hdfs hadoop fs -chown mapred:hadoop /user/history

#create log directories
sudo -u hdfs hadoop fs -mkdir -p /var/log/hadoop-yarn
sudo -u hdfs hadoop fs -chown yarn:mapred /var/log/hadoop-yarn




############################### STARTING SERVICES ######################3

#Start Hdfs 
for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do sudo service $x start ; done

sudo service hadoop-yarn-resourcemanager start
sudo service hadoop-yarn-nodemanager start
sudo service hadoop-mapreduce-historyserver start

### MONGO HADOOP ###### 
cd /usr/lib
./gradlew -PclusterVersion='cdh5' assemble
cp /usr/lib/mongo-hadoop-r1.3.0/core/build/libs/mongo-hadoop-core-1.3.0.jar /usr/lib/hadoop/lib/.
cp /usr/lib/mongo-hadoop-r1.3.0/hive/build/libs/mongo-hadoop-hive-1.3.0.jar /usr/lib/hadoop/lib/.
cp mongo-java-driver-2.12.2.jar /usr/lib/hadoop/lib/.



################## HIVE ##################
sudo yum install hive hive-metastore hive-server2

#example1 - network
CREATE EXTERNAL TABLE userActivity
( 
  userId INT,
  type STRING,
  date STRING
  
)
STORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler'
WITH SERDEPROPERTIES('mongo.columns.mapping'='{"userId":"userId","type":"type","date":"date"}')
TBLPROPERTIES('mongo.uri'='mongodb://business:password@fintonicdev1.cloudapp.net:27017/business.userActivity');


#example1.a - replicarset
CREATE EXTERNAL TABLE userActivity
( 
  userId INT,
  type STRING,
  date STRING
  
)
STORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler'
WITH SERDEPROPERTIES('mongo.columns.mapping'='{"userId":"userId","type":"type","date":"date"}')
TBLPROPERTIES('mongo.uri'='mongodb://business:password@10.0.101.220,10.0.101.195:27017/business.userActivity?readPreference=secondary');


#example query
hive -e "select type,date,count(*) as quant from event group by type,date order by date,quant" > leanOut.csv

#example 2 - file
CREATE EXTERNAL TABLE userActivity
( 
  userId INT,
  type STRING,
  date STRING
  
)
ROW FORMAT SERDE "com.mongodb.hadoop.hive.BSONSerDe"
WITH SERDEPROPERTIES('mongo.columns.mapping'='{"userId":"userId","type":"type","date":"date"}')
STORED AS INPUTFORMAT "com.mongodb.hadoop.mapred.BSONFileInputFormat"
OUTPUTFORMAT "com.mongodb.hadoop.hive.output.HiveBSONFileOutputFormat"
LOCATION 'hdfs://fintonicdev1.cloudapp.net:8020/user/root/bson/';




db.grantRolesToUser(
  "business",
  [   
    {
      role: "clusterManager", db:"admin"
    }
  ]
)

db.grantRolesToUser(
  "business",
  [   
    {
      role: "clusterManager", db:"admin"
    }
  ]
)




################ ZOOKEEPER ###########
sudo yum install zookeeper
sudo yum install zookeeper-server
sudo service zookeeper-server init
sudo service zookeeper-server start



############### KAFKA ####################


wget http://apache.rediris.es/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz
Untar and now start the server

bin/kafka-server-start.sh config/server.properties



##### OPLOG ############
db.createUser({ user: "monstream",
  pwd: "monstream",  
  roles: [
    { role: "read", db: "local" }    
  ]
})



############ SBT ###########

g8 typesafehub/scala-sbt
cd scala-project
sbt eclipse

//create build.sbt in root directory and add dependencies


############# SPARK ###############

//FOR DEV
SPARK_CLASSPATH="/usr/lib/mongo-hadoop-r1.3.0/core/build/libs/mongo-hadoop-core-1.3.0.jar:/usr/lib/mongo-hadoop-r1.3.0/hive/build/libs/mongo-hadoop-hive-1.3.0.jar:/usr/lib/hadoop/lib/mongo-java-driver-2.12.2.jar" ./bin/spark-shell

//FOR EMR
/home/hadoop/spark/bin/spark-shell

cp hive-default.xml /home/hadoop/spark/conf/hive-site.xml
cp /home/hadoop/lib/mongo-* /home/hadoop/spark/classpath/emr/.